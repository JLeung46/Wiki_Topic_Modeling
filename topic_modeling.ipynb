{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "PUNCTUATION = set(string.punctuation)\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wiki MappedRDD[1] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_rdd = sc.textFile('s3n://wikisample10/sample2')\n",
    "wiki_rdd.cache()\n",
    "wiki_rdd.setName('wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1524144"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = wiki_rdd.count()\n",
    "f = open('count.txt', 'w')\n",
    "f.write(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Take 10 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki_rdd_samples = sc.parallelize(wiki_rdd.take(10), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Filter out articles that start with #REDIRECT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'#REDIRECT [[Computer accessibility]]  {{Redr|move|from CamelCase|up}}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_rdd_samples.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki_no_redirect_rdd = wiki_rdd_samples.filter(lambda line: '#REDIRECT' not in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'{{Redirect2|Anarchist|Anarchists|the fictional character|Anarchist (comics)|other uses|Anarchists (d'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_redirect = wiki_no_redirect_rdd.first()\n",
    "no_redirect[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Token and Stem each article**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    regex = re.compile('<.+?>|[^a-zA-Z]')\n",
    "    clean_txt = regex.sub(' ', text)\n",
    "    tokens = clean_txt.split()\n",
    "    lowercased = [t.lower() for t in tokens]\n",
    "\n",
    "    no_punctuation = []\n",
    "    for word in lowercased:\n",
    "        punct_removed = ''.join([letter for letter in word if not letter in PUNCTUATION])\n",
    "        no_punctuation.append(punct_removed)\n",
    "    no_stopwords = [w for w in no_punctuation if not w in STOPWORDS]\n",
    "    \n",
    "    STEMMER = PorterStemmer()\n",
    "    stemmed = [STEMMER.stem(w) for w in no_stopwords]\n",
    "    return [w for w in stemmed if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_rdd = wiki_no_redirect_rdd.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'redirect',\n",
       " u'anarchist',\n",
       " u'anarchist',\n",
       " u'fiction',\n",
       " u'charact',\n",
       " u'anarchist',\n",
       " u'comic',\n",
       " u'use',\n",
       " u'anarchist',\n",
       " u'disambigu']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = token_rdd.first()\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Compute TF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = token_rdd.flatMap(lambda x: x).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tf(word_lst):\n",
    "    count_of_each_word = Counter(word_lst)\n",
    "    doc_word_count = len(word_lst) * 1.\n",
    "    return np.array([count_of_each_word[v] / doc_word_count if v in count_of_each_word else 0 for v in vocab])\n",
    "    \n",
    "tf_rdd = token_rdd.map(get_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0002592352559948153,\n",
       " 0.0005184705119896306,\n",
       " 0.0003888528839922229,\n",
       " 0,\n",
       " 0.0005184705119896306,\n",
       " 0.00019442644199611146,\n",
       " 6.480881399870383e-05,\n",
       " 6.480881399870383e-05,\n",
       " 0.00012961762799740766,\n",
       " 0,\n",
       " 0.0005184705119896306,\n",
       " 6.480881399870383e-05,\n",
       " 6.480881399870383e-05,\n",
       " 0,\n",
       " 0.00012961762799740766,\n",
       " 6.480881399870383e-05,\n",
       " 6.480881399870383e-05,\n",
       " 0.00012961762799740766,\n",
       " 0.0003240440699935191,\n",
       " 6.480881399870383e-05]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_first = tf_rdd.first()\n",
    "tf_first[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Compute IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_doc_count = tf_rdd.count()\n",
    "times_words_in_doc = tf_rdd.map(lambda tf_lst: ((np.array(tf_lst) > 0) + 0)).sum()\n",
    "idf = np.log(total_doc_count / times_words_in_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Compute TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfidf PythonRDD[36] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_rdd = tf_rdd.map(lambda tf_vec: tf_vec * idf)\n",
    "tfidf_rdd.cache()\n",
    "tfidf_rdd.setName('tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Run K-Means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = KMeans.train(tfidf_rdd, 10)\n",
    "centriods = model.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  1.79688187e-04,   0.00000000e+00,   2.69532280e-04, ...,\n",
       "          4.49220467e-05,   0.00000000e+00,   0.00000000e+00]),\n",
       " array([ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.00021243])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centriods"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
